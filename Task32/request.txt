Step tiếp theo: deploy model
Tutorials:
Gọi API: https://github.com/triton-inference-server/tutorials/blob/main/Quick_Deploy/ONNX/README.md
Hosting: https://github.com/triton-inference-server/server/blob/main/docs/getting_started/quickstart.md
Đo performance: https://github.com/triton-inference-server/server/blob/main/docs/perf_benchmark/perf-analyzer-README.rst

Reference: 
https://github.com/triton-inference-server/server?tab=readme-ov-file
https://github.com/triton-inference-server/tutorials/tree/main

Yêu cầu:
down được image của triton server và hosting
viết file config để hosting model
dùng Tutorials link 1 để gọi API và chạy inference -> ra được kết quả là done
push code github (có gì push nấy)
(Extra - 10đ) Đo performance với toàn bộ config có thể chỉnh sửa từ 2)  - Tutorials Link 3
